"""
Tool: Compare model answers with benchmark answers
"""

from typing import List, Dict
import logging
from difflib import SequenceMatcher

logger = logging.getLogger(__name__)


class AnswerComparator:
    """
    Compare model-generated answers with benchmark correct answers
    """
    
    def __init__(self, config=None):
        """
        Initialize Answer Comparator
        
        Args:
            config: Configuration object
        """
        self.config = config
        self.similarity_threshold = 0.8
        logger.info("AnswerComparator initialized")
    
    def compare(
        self,
        model_answers: List[str],
        benchmark_answers: List[str],
        questions: List[Dict]
    ) -> Dict:
        """
        Compare model answers with benchmark answers
        
        Args:
            model_answers: Answers generated by model
            benchmark_answers: Correct benchmark answers
            questions: Original questions
            
        Returns:
            Dictionary with comparison results
        """
        logger.info(f"Comparing {len(model_answers)} answer pairs")
        
        if len(model_answers) != len(benchmark_answers):
            raise ValueError(
                f"Answer count mismatch: {len(model_answers)} vs {len(benchmark_answers)}"
            )
        
        correct_indices = []
        incorrect_indices = []
        detailed_comparisons = []
        
        for i, (model_ans, bench_ans) in enumerate(zip(model_answers, benchmark_answers)):
            # Compare answers
            is_correct = self._is_correct(model_ans, bench_ans)
            
            # Calculate similarity score
            similarity = self._calculate_similarity(model_ans, bench_ans)
            
            comparison = {
                "index": i,
                "question_id": questions[i].get("question_id", f"Q{i+1}"),
                "model_answer": model_ans,
                "benchmark_answer": bench_ans,
                "is_correct": is_correct,
                "similarity_score": similarity,
                "domain": questions[i].get("domain", "unknown"),
                "difficulty": questions[i].get("difficulty", "medium")
            }
            
            detailed_comparisons.append(comparison)
            
            if is_correct:
                correct_indices.append(i)
            else:
                incorrect_indices.append(i)
        
        results = {
            "correct_indices": correct_indices,
            "incorrect_indices": incorrect_indices,
            "correct_count": len(correct_indices),
            "incorrect_count": len(incorrect_indices),
            "total_count": len(model_answers),
            "accuracy": len(correct_indices) / len(model_answers) if model_answers else 0,
            "detailed_comparisons": detailed_comparisons
        }
        
        logger.info(f"Comparison completed: {len(correct_indices)} correct, {len(incorrect_indices)} incorrect")
        
        return results
    
    def _is_correct(self, model_answer: str, benchmark_answer: str) -> bool:
        """
        Determine if model answer is correct
        
        Args:
            model_answer: Model's answer
            benchmark_answer: Correct answer
            
        Returns:
            True if correct, False otherwise
        """
        # Normalize answers
        model_norm = self._normalize_answer(model_answer)
        bench_norm = self._normalize_answer(benchmark_answer)
        
        # Exact match
        if model_norm == bench_norm:
            return True
        
        # Check if it's a multiple choice answer (A, B, C, D)
        if len(model_norm) == 1 and len(bench_norm) == 1:
            return model_norm == bench_norm
        
        # Check for similarity (for free-text answers)
        similarity = self._calculate_similarity(model_norm, bench_norm)
        return similarity >= self.similarity_threshold
    
    def _normalize_answer(self, answer: str) -> str:
        """
        Normalize answer for comparison
        
        Args:
            answer: Raw answer string
            
        Returns:
            Normalized answer
        """
        # Convert to lowercase
        normalized = answer.lower().strip()
        
        # Extract letter if it's a multiple choice answer
        # e.g., "B)