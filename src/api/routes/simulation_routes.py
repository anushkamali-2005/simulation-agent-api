"""
Simulation Agent API Routes
"""

from typing import List, Dict, Any
from fastapi import APIRouter, HTTPException, status
import logging
import uuid
from datetime import datetime

from src.api.schemas import (
    QuestionGenerateRequest,
    QuestionGenerateResponse,
    QuestionResponse,
    BenchmarkLoadRequest,
    BenchmarkLoadResponse,
    CompareAnswersRequest,
    ComparisonResult,
    DetailedComparison,
    SimulationRunRequest,
    SimulationResponse,
    MetricsResponse,
    ErrorAnalysisResponse,
    ErrorExample,
    HealthResponse,
    ErrorResponse,
)

# Import simulation agent tools
from agents.agent_2_simulation.tools.question_generator import QuestionGenerator
from agents.agent_2_simulation.tools.benchmark_loader import BenchmarkLoader
from agents.agent_2_simulation.tools.answer_comparator import AnswerComparator

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/simulation", tags=["Simulation"])

# Initialize tools
question_generator = QuestionGenerator()
benchmark_loader = BenchmarkLoader()
answer_comparator = AnswerComparator()


@router.get("/health", response_model=HealthResponse)
async def health_check():
    """
    Health check endpoint
    
    Returns API status and version information
    """
    return HealthResponse(
        status="healthy",
        version="1.0.0",
        agent="Simulation Agent",
        message="API is running successfully"
    )


@router.post("/generate-questions", response_model=QuestionGenerateResponse)
async def generate_questions(request: QuestionGenerateRequest):
    """
    Generate clinical simulation questions
    
    - **num_questions**: Number of questions to generate (1-500)
    - **difficulty**: Question difficulty (easy, medium, hard, varied)
    - **domains**: List of medical domains (optional)
    - **model_type**: Type of model being tested
    
    Returns a list of generated questions with answers and explanations
    """
    try:
        logger.info(f"Generating {request.num_questions} questions")
        
        # Generate questions
        questions = question_generator.generate(
            num_questions=request.num_questions,
            difficulty=request.difficulty.value,
            domains=request.domains,
            model_type=request.model_type
        )
        
        # Convert to response format
        question_responses = [
            QuestionResponse(**q) for q in questions
        ]
        
        return QuestionGenerateResponse(
            questions=question_responses,
            count=len(question_responses),
            message=f"Successfully generated {len(question_responses)} questions"
        )
        
    except Exception as e:
        logger.error(f"Error generating questions: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate questions: {str(e)}"
        )


@router.post("/load-benchmarks", response_model=BenchmarkLoadResponse)
async def load_benchmarks(request: BenchmarkLoadRequest):
    """
    Load benchmark (correct) answers for questions
    
    - **questions**: List of question dictionaries
    - **source**: Source for benchmark answers (auto, questions, file, medagentgym)
    
    Returns benchmark answers aligned with the provided questions
    """
    try:
        logger.info(f"Loading benchmarks for {len(request.questions)} questions")
        
        # Load benchmark answers
        benchmark_answers = benchmark_loader.load_benchmark_answers(
            questions=request.questions,
            source=request.source.value
        )
        
        return BenchmarkLoadResponse(
            benchmark_answers=benchmark_answers,
            count=len(benchmark_answers),
            source_used=request.source.value,
            message=f"Successfully loaded {len(benchmark_answers)} benchmark answers"
        )
        
    except Exception as e:
        logger.error(f"Error loading benchmarks: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to load benchmarks: {str(e)}"
        )


@router.post("/compare-answers", response_model=ComparisonResult)
async def compare_answers(request: CompareAnswersRequest):
    """
    Compare model answers with benchmark answers
    
    - **model_answers**: Answers generated by the model
    - **benchmark_answers**: Correct benchmark answers
    - **questions**: Original questions for context
    
    Returns detailed comparison results with accuracy metrics
    """
    try:
        logger.info(f"Comparing {len(request.model_answers)} answer pairs")
        
        # Validate input
        if len(request.model_answers) != len(request.benchmark_answers):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Answer count mismatch: {len(request.model_answers)} model answers vs {len(request.benchmark_answers)} benchmark answers"
            )
        
        if len(request.model_answers) != len(request.questions):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Question count mismatch: {len(request.questions)} questions vs {len(request.model_answers)} answers"
            )
        
        # Compare answers
        comparison_results = answer_comparator.compare(
            model_answers=request.model_answers,
            benchmark_answers=request.benchmark_answers,
            questions=request.questions
        )
        
        # Convert to response format
        detailed_comparisons = [
            DetailedComparison(**comp) for comp in comparison_results["detailed_comparisons"]
        ]
        
        return ComparisonResult(
            correct_indices=comparison_results["correct_indices"],
            incorrect_indices=comparison_results["incorrect_indices"],
            correct_count=comparison_results["correct_count"],
            incorrect_count=comparison_results["incorrect_count"],
            total_count=comparison_results["total_count"],
            accuracy=comparison_results["accuracy"],
            detailed_comparisons=detailed_comparisons
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error comparing answers: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to compare answers: {str(e)}"
        )


@router.post("/run", response_model=SimulationResponse)
async def run_simulation(request: SimulationRunRequest):
    """
    Run a complete simulation workflow
    
    This endpoint orchestrates the entire simulation process:
    1. Generate questions (or use provided questions)
    2. Load benchmark answers
    3. Compare with model answers (if provided)
    4. Calculate metrics
    5. Analyze errors
    
    - **num_questions**: Number of questions to generate
    - **difficulty**: Question difficulty level
    - **domains**: Medical domains to test
    - **model_type**: Type of model
    - **model_name**: Name of the model being tested
    - **questions**: Optional pre-generated questions
    - **model_answers**: Optional model answers for comparison
    
    Returns complete simulation results with metrics and error analysis
    """
    try:
        session_id = str(uuid.uuid4())
        logger.info(f"Starting simulation {session_id}")
        
        warnings = []
        errors = []
        
        # Step 1: Generate or use provided questions
        if request.questions:
            questions = request.questions
            logger.info(f"Using {len(questions)} provided questions")
        else:
            logger.info(f"Generating {request.num_questions} questions")
            questions = question_generator.generate(
                num_questions=request.num_questions,
                difficulty=request.difficulty.value,
                domains=request.domains,
                model_type=request.model_type
            )
        
        # Step 2: Load benchmark answers
        logger.info("Loading benchmark answers")
        benchmark_answers = benchmark_loader.load_benchmark_answers(
            questions=questions,
            source="auto"
        )
        
        # Convert questions to response format
        question_responses = [QuestionResponse(**q) for q in questions]
        
        # Step 3: Compare answers if model answers provided
        comparison_results = None
        metrics = None
        error_analysis = None
        simulation_passed = False
        simulation_accuracy = 0.0
        
        if request.model_answers:
            logger.info("Comparing model answers")
            
            if len(request.model_answers) != len(questions):
                warnings.append(
                    f"Model answer count ({len(request.model_answers)}) does not match question count ({len(questions)})"
                )
            else:
                # Compare answers
                comp_results = answer_comparator.compare(
                    model_answers=request.model_answers,
                    benchmark_answers=benchmark_answers,
                    questions=questions
                )
                
                detailed_comparisons = [
                    DetailedComparison(**comp) for comp in comp_results["detailed_comparisons"]
                ]
                
                comparison_results = ComparisonResult(
                    correct_indices=comp_results["correct_indices"],
                    incorrect_indices=comp_results["incorrect_indices"],
                    correct_count=comp_results["correct_count"],
                    incorrect_count=comp_results["incorrect_count"],
                    total_count=comp_results["total_count"],
                    accuracy=comp_results["accuracy"],
                    detailed_comparisons=detailed_comparisons
                )
                
                # Calculate metrics
                simulation_accuracy = comp_results["accuracy"]
                
                # Calculate domain and difficulty breakdowns
                accuracy_by_domain = {}
                accuracy_by_difficulty = {}
                
                for comp in comp_results["detailed_comparisons"]:
                    domain = comp["domain"]
                    difficulty = comp["difficulty"]
                    
                    if domain not in accuracy_by_domain:
                        accuracy_by_domain[domain] = {"correct": 0, "total": 0}
                    if difficulty not in accuracy_by_difficulty:
                        accuracy_by_difficulty[difficulty] = {"correct": 0, "total": 0}
                    
                    accuracy_by_domain[domain]["total"] += 1
                    accuracy_by_difficulty[difficulty]["total"] += 1
                    
                    if comp["is_correct"]:
                        accuracy_by_domain[domain]["correct"] += 1
                        accuracy_by_difficulty[difficulty]["correct"] += 1
                
                # Calculate percentages
                domain_accuracy = {
                    domain: stats["correct"] / stats["total"] if stats["total"] > 0 else 0
                    for domain, stats in accuracy_by_domain.items()
                }
                difficulty_accuracy = {
                    diff: stats["correct"] / stats["total"] if stats["total"] > 0 else 0
                    for diff, stats in accuracy_by_difficulty.items()
                }
                
                metrics = MetricsResponse(
                    accuracy=simulation_accuracy,
                    correct_count=comp_results["correct_count"],
                    incorrect_count=comp_results["incorrect_count"],
                    total_count=comp_results["total_count"],
                    accuracy_by_domain=domain_accuracy,
                    accuracy_by_difficulty=difficulty_accuracy
                )
                
                # Analyze errors if there are any
                if comp_results["incorrect_count"] > 0:
                    logger.info("Analyzing errors")
                    
                    # Simple error categorization
                    error_types = {}
                    error_examples = []
                    
                    for comp in comp_results["detailed_comparisons"]:
                        if not comp["is_correct"]:
                            # Categorize by domain
                            error_type = f"{comp['domain']}_error"
                            error_types[error_type] = error_types.get(error_type, 0) + 1
                            
                            # Add to examples (limit to 5)
                            if len(error_examples) < 5:
                                q = questions[comp["index"]]
                                error_examples.append(
                                    ErrorExample(
                                        question_id=comp["question_id"],
                                        question_text=q.get("question_text", ""),
                                        model_answer=comp["model_answer"],
                                        correct_answer=comp["benchmark_answer"],
                                        error_type=error_type,
                                        domain=comp["domain"],
                                        difficulty=comp["difficulty"]
                                    )
                                )
                    
                    # Generate improvement suggestions
                    suggestions = []
                    if simulation_accuracy < 0.7:
                        suggestions.append("Model accuracy is below 70%. Consider additional training or fine-tuning.")
                    
                    most_common_error = max(error_types.items(), key=lambda x: x[1])[0] if error_types else None
                    if most_common_error:
                        domain = most_common_error.replace("_error", "")
                        suggestions.append(f"Focus on improving {domain} domain knowledge - highest error rate detected here.")
                    
                    if difficulty_accuracy.get("hard", 0) < 0.5:
                        suggestions.append("Performance on hard questions is weak. Consider more challenging training data.")
                    
                    error_analysis = ErrorAnalysisResponse(
                        total_errors=comp_results["incorrect_count"],
                        error_types=error_types,
                        error_examples=error_examples,
                        improvement_suggestions=suggestions
                    )
                
                # Determine if simulation passed (e.g., 80% threshold)
                simulation_passed = simulation_accuracy >= 0.8
        else:
            warnings.append("No model answers provided - skipping comparison and metrics calculation")
        
        # Build response
        message = f"Simulation completed successfully with {len(questions)} questions"
        if simulation_accuracy > 0:
            message += f" - Accuracy: {simulation_accuracy:.2%}"
        
        return SimulationResponse(
            session_id=session_id,
            status="completed",
            questions=question_responses,
            benchmark_answers=benchmark_answers,
            model_answers=request.model_answers,
            comparison_results=comparison_results,
            metrics=metrics,
            error_analysis=error_analysis,
            simulation_passed=simulation_passed,
            simulation_accuracy=simulation_accuracy,
            message=message,
            warnings=warnings,
            errors=errors
        )
        
    except Exception as e:
        logger.error(f"Error running simulation: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to run simulation: {str(e)}"
        )
