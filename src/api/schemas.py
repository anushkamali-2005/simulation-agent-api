"""
Pydantic schemas for Simulation Agent API
"""

from typing import List, Dict, Optional, Any
from pydantic import BaseModel, Field
from enum import Enum


class DifficultyLevel(str, Enum):
    """Question difficulty levels"""
    EASY = "easy"
    MEDIUM = "medium"
    HARD = "hard"
    VARIED = "varied"


class QuestionType(str, Enum):
    """Types of questions"""
    MULTIPLE_CHOICE = "multiple_choice"
    FREE_TEXT = "free_text"
    TRUE_FALSE = "true_false"


class BenchmarkSource(str, Enum):
    """Sources for benchmark answers"""
    AUTO = "auto"
    QUESTIONS = "questions"
    FILE = "file"
    MEDAGENTGYM = "medagentgym"


# ============================================================================
# Request Models
# ============================================================================

class QuestionGenerateRequest(BaseModel):
    """Request to generate clinical questions"""
    num_questions: int = Field(default=50, ge=1, le=500, description="Number of questions to generate")
    difficulty: DifficultyLevel = Field(default=DifficultyLevel.VARIED, description="Question difficulty level")
    domains: Optional[List[str]] = Field(default=None, description="Medical domains (e.g., cardiology, neurology)")
    model_type: str = Field(default="general", description="Type of model being tested")

    class Config:
        json_schema_extra = {
            "example": {
                "num_questions": 10,
                "difficulty": "varied",
                "domains": ["cardiology", "neurology"],
                "model_type": "general"
            }
        }


class BenchmarkLoadRequest(BaseModel):
    """Request to load benchmark answers"""
    questions: List[Dict[str, Any]] = Field(..., description="List of question dictionaries")
    source: BenchmarkSource = Field(default=BenchmarkSource.AUTO, description="Source for benchmark answers")

    class Config:
        json_schema_extra = {
            "example": {
                "questions": [
                    {
                        "question_id": "Q001",
                        "question_text": "Sample question?",
                        "correct_answer": "A) Option A"
                    }
                ],
                "source": "auto"
            }
        }


class CompareAnswersRequest(BaseModel):
    """Request to compare model answers with benchmarks"""
    model_answers: List[str] = Field(..., description="Answers generated by the model")
    benchmark_answers: List[str] = Field(..., description="Correct benchmark answers")
    questions: List[Dict[str, Any]] = Field(..., description="Original questions")

    class Config:
        json_schema_extra = {
            "example": {
                "model_answers": ["A) Option A", "B) Option B"],
                "benchmark_answers": ["A) Option A", "C) Option C"],
                "questions": [
                    {"question_id": "Q001", "domain": "cardiology", "difficulty": "easy"},
                    {"question_id": "Q002", "domain": "neurology", "difficulty": "medium"}
                ]
            }
        }


class SimulationRunRequest(BaseModel):
    """Request to run a complete simulation"""
    num_questions: int = Field(default=50, ge=1, le=500, description="Number of questions")
    difficulty: DifficultyLevel = Field(default=DifficultyLevel.VARIED, description="Question difficulty")
    domains: Optional[List[str]] = Field(default=None, description="Medical domains to test")
    model_type: str = Field(default="general", description="Model type")
    model_name: Optional[str] = Field(default="SimulationModel", description="Name of the model being tested")
    
    # Optional: provide pre-generated questions
    questions: Optional[List[Dict[str, Any]]] = Field(default=None, description="Pre-generated questions (optional)")
    
    # Optional: provide model answers for comparison
    model_answers: Optional[List[str]] = Field(default=None, description="Model answers (optional, for testing)")

    class Config:
        json_schema_extra = {
            "example": {
                "num_questions": 20,
                "difficulty": "varied",
                "domains": ["cardiology", "emergency_medicine"],
                "model_type": "general",
                "model_name": "MedLM-v1"
            }
        }


# ============================================================================
# Response Models
# ============================================================================

class QuestionResponse(BaseModel):
    """Individual question structure"""
    question_id: str
    question_text: str
    domain: str
    difficulty: str
    question_type: str
    options: List[str]
    correct_answer: str
    explanation: str
    metadata: Dict[str, Any] = Field(default_factory=dict)


class QuestionGenerateResponse(BaseModel):
    """Response from question generation"""
    questions: List[QuestionResponse]
    count: int
    message: str = "Questions generated successfully"


class BenchmarkLoadResponse(BaseModel):
    """Response from benchmark loading"""
    benchmark_answers: List[str]
    count: int
    source_used: str
    message: str = "Benchmark answers loaded successfully"


class DetailedComparison(BaseModel):
    """Detailed comparison for a single question"""
    index: int
    question_id: str
    model_answer: str
    benchmark_answer: str
    is_correct: bool
    similarity_score: float
    domain: str
    difficulty: str


class ComparisonResult(BaseModel):
    """Result of answer comparison"""
    correct_indices: List[int]
    incorrect_indices: List[int]
    correct_count: int
    incorrect_count: int
    total_count: int
    accuracy: float
    detailed_comparisons: List[DetailedComparison]


class MetricsResponse(BaseModel):
    """Performance metrics"""
    accuracy: float
    correct_count: int
    incorrect_count: int
    total_count: int
    accuracy_by_domain: Dict[str, float] = Field(default_factory=dict)
    accuracy_by_difficulty: Dict[str, float] = Field(default_factory=dict)


class ErrorExample(BaseModel):
    """Example of an error"""
    question_id: str
    question_text: str
    model_answer: str
    correct_answer: str
    error_type: str
    domain: str
    difficulty: str


class ErrorAnalysisResponse(BaseModel):
    """Error analysis results"""
    total_errors: int
    error_types: Dict[str, int]
    error_examples: List[ErrorExample]
    improvement_suggestions: List[str]


class SimulationResponse(BaseModel):
    """Complete simulation results"""
    session_id: str
    status: str
    questions: List[QuestionResponse]
    benchmark_answers: List[str]
    model_answers: Optional[List[str]] = None
    comparison_results: Optional[ComparisonResult] = None
    metrics: Optional[MetricsResponse] = None
    error_analysis: Optional[ErrorAnalysisResponse] = None
    simulation_passed: bool = False
    simulation_accuracy: float = 0.0
    message: str
    warnings: List[str] = Field(default_factory=list)
    errors: List[str] = Field(default_factory=list)


class HealthResponse(BaseModel):
    """Health check response"""
    status: str = "healthy"
    version: str = "1.0.0"
    agent: str = "Simulation Agent"
    message: str = "API is running"


class ErrorResponse(BaseModel):
    """Error response"""
    error: str
    detail: Optional[str] = None
    status_code: int
